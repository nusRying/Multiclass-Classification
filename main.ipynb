{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1286742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 10015/10015 [40:27<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction completed successfully.\n",
      "LBP  file saved: ham10000_lbp_multiclass.csv  | Shape: (10015, 19)\n",
      "GLCM file saved: ham10000_glcm_multiclass.csv | Shape: (10015, 41)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "\n",
    "# =====================================================\n",
    "# CONFIG\n",
    "# =====================================================\n",
    "\n",
    "DATASET = \"HAM10000\"   # \"HAM10000\" or \"ISIC2019\"\n",
    "CLEAN_DATA_ROOT = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Code\\CleanData\"\n",
    "\n",
    "# =====================================================\n",
    "# DATASET PATHS & CLASS MAPS\n",
    "# =====================================================\n",
    "\n",
    "if DATASET == \"HAM10000\":\n",
    "    IMAGE_DIR = os.path.join(CLEAN_DATA_ROOT, \"HAM10000\", \"images\")\n",
    "    LABEL_CSV = os.path.join(CLEAN_DATA_ROOT, \"HAM10000\", \"HAM10000_metadata\")\n",
    "\n",
    "    CLASS_MAP = {\n",
    "        \"akiec\": 0,\n",
    "        \"bcc\": 1,\n",
    "        \"bkl\": 2,\n",
    "        \"df\": 3,\n",
    "        \"mel\": 4,\n",
    "        \"nv\": 5,\n",
    "        \"vasc\": 6\n",
    "    }\n",
    "\n",
    "    LBP_OUT = \"ham10000_lbp_multiclass.csv\"\n",
    "    GLCM_OUT = \"ham10000_glcm_multiclass.csv\"\n",
    "\n",
    "elif DATASET == \"ISIC2019\":\n",
    "    IMAGE_DIR = os.path.join(CLEAN_DATA_ROOT, \"ISIC2019\", \"images_train\")\n",
    "    LABEL_CSV = os.path.join(CLEAN_DATA_ROOT, \"ISIC2019\", \"ISIC_2019_Training_GroundTruth.csv\")\n",
    "\n",
    "    CLASS_MAP = {\n",
    "        \"AK\": 0,\n",
    "        \"BCC\": 1,\n",
    "        \"BKL\": 2,\n",
    "        \"DF\": 3,\n",
    "        \"MEL\": 4,\n",
    "        \"NV\": 5,\n",
    "        \"SCC\": 6,\n",
    "        \"VASC\": 7\n",
    "    }\n",
    "\n",
    "    LBP_OUT = \"isic2019_lbp_multiclass.csv\"\n",
    "    GLCM_OUT = \"isic2019_glcm_multiclass.csv\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"DATASET must be 'HAM10000' or 'ISIC2019'\")\n",
    "\n",
    "# =====================================================\n",
    "# LBP CONFIG\n",
    "# =====================================================\n",
    "\n",
    "LBP_RADIUS = 2\n",
    "LBP_POINTS = 8 * LBP_RADIUS\n",
    "LBP_METHOD = \"uniform\"\n",
    "\n",
    "# =====================================================\n",
    "# GLCM CONFIG\n",
    "# =====================================================\n",
    "\n",
    "GLCM_DISTANCES = [1, 2]\n",
    "GLCM_ANGLES = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "GLCM_PROPS = [\"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\", \"correlation\"]\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def extract_lbp(gray):\n",
    "    lbp = local_binary_pattern(gray, LBP_POINTS, LBP_RADIUS, method=LBP_METHOD)\n",
    "    hist, _ = np.histogram(\n",
    "        lbp.ravel(),\n",
    "        bins=np.arange(0, LBP_POINTS + 3),\n",
    "        range=(0, LBP_POINTS + 2),\n",
    "        density=True\n",
    "    )\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "def extract_glcm(gray):\n",
    "    glcm = graycomatrix(\n",
    "        gray,\n",
    "        distances=GLCM_DISTANCES,\n",
    "        angles=GLCM_ANGLES,\n",
    "        levels=256,\n",
    "        symmetric=True,\n",
    "        normed=True\n",
    "    )\n",
    "\n",
    "    feats = []\n",
    "    for prop in GLCM_PROPS:\n",
    "        feats.extend(graycoprops(glcm, prop).ravel())\n",
    "\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "# =====================================================\n",
    "# LOAD LABELS\n",
    "# =====================================================\n",
    "\n",
    "labels_df = pd.read_csv(LABEL_CSV)\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE EXTRACTION\n",
    "# =====================================================\n",
    "\n",
    "lbp_rows = []\n",
    "glcm_rows = []\n",
    "\n",
    "for _, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"Extracting features\"):\n",
    "\n",
    "    # ---------- LABEL ----------\n",
    "    if DATASET == \"HAM10000\":\n",
    "        image_id = row[\"image_id\"]\n",
    "        label = CLASS_MAP[row[\"dx\"]]\n",
    "    else:\n",
    "        image_id = row[\"image\"]\n",
    "        label_name = max(CLASS_MAP, key=lambda c: row[c])\n",
    "        label = CLASS_MAP[label_name]\n",
    "\n",
    "    # ---------- IMAGE ----------\n",
    "    img_path = os.path.join(IMAGE_DIR, image_id + \".jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ---------- FEATURES ----------\n",
    "    lbp_feat = extract_lbp(gray)\n",
    "    glcm_feat = extract_glcm(gray)\n",
    "\n",
    "    lbp_rows.append(np.concatenate([lbp_feat, [label]]))\n",
    "    glcm_rows.append(np.concatenate([glcm_feat, [label]]))\n",
    "\n",
    "# =====================================================\n",
    "# SAVE LBP CSV\n",
    "# =====================================================\n",
    "\n",
    "lbp_feature_names = [f\"lbp_{i}\" for i in range(len(lbp_feat))] + [\"label\"]\n",
    "lbp_df = pd.DataFrame(lbp_rows, columns=lbp_feature_names)\n",
    "lbp_df.to_csv(LBP_OUT, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# SAVE GLCM CSV\n",
    "# =====================================================\n",
    "\n",
    "glcm_feature_names = [f\"glcm_{i}\" for i in range(len(glcm_feat))] + [\"label\"]\n",
    "glcm_df = pd.DataFrame(glcm_rows, columns=glcm_feature_names)\n",
    "glcm_df.to_csv(GLCM_OUT, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# DONE\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\nFeature extraction completed successfully.\")\n",
    "print(f\"LBP  file saved: {LBP_OUT}  | Shape: {lbp_df.shape}\")\n",
    "print(f\"GLCM file saved: {GLCM_OUT} | Shape: {glcm_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ecd6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 25331/25331 [2:17:26<00:00,  3.07it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction completed successfully.\n",
      "LBP  file saved: isic2019_lbp_multiclass.csv  | Shape: (25331, 19)\n",
      "GLCM file saved: isic2019_glcm_multiclass.csv | Shape: (25331, 41)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "\n",
    "# =====================================================\n",
    "# CONFIG\n",
    "# =====================================================\n",
    "\n",
    "DATASET = \"ISIC2019\"   # \"HAM10000\" or \"ISIC2019\"\n",
    "CLEAN_DATA_ROOT = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Code\\CleanData\"\n",
    "\n",
    "# =====================================================\n",
    "# DATASET PATHS & CLASS MAPS\n",
    "# =====================================================\n",
    "\n",
    "if DATASET == \"HAM10000\":\n",
    "    IMAGE_DIR = os.path.join(CLEAN_DATA_ROOT, \"HAM10000\", \"images\")\n",
    "    LABEL_CSV = os.path.join(CLEAN_DATA_ROOT, \"HAM10000\", \"HAM10000_metadata\")\n",
    "\n",
    "    CLASS_MAP = {\n",
    "        \"akiec\": 0,\n",
    "        \"bcc\": 1,\n",
    "        \"bkl\": 2,\n",
    "        \"df\": 3,\n",
    "        \"mel\": 4,\n",
    "        \"nv\": 5,\n",
    "        \"vasc\": 6\n",
    "    }\n",
    "\n",
    "    LBP_OUT = \"ham10000_lbp_multiclass.csv\"\n",
    "    GLCM_OUT = \"ham10000_glcm_multiclass.csv\"\n",
    "\n",
    "elif DATASET == \"ISIC2019\":\n",
    "    IMAGE_DIR = os.path.join(CLEAN_DATA_ROOT, \"ISIC2019\", \"images_train\")\n",
    "    LABEL_CSV = os.path.join(CLEAN_DATA_ROOT, \"ISIC2019\", \"ISIC_2019_Training_GroundTruth.csv\")\n",
    "\n",
    "    CLASS_MAP = {\n",
    "        \"AK\": 0,\n",
    "        \"BCC\": 1,\n",
    "        \"BKL\": 2,\n",
    "        \"DF\": 3,\n",
    "        \"MEL\": 4,\n",
    "        \"NV\": 5,\n",
    "        \"SCC\": 6,\n",
    "        \"VASC\": 7\n",
    "    }\n",
    "\n",
    "    LBP_OUT = \"isic2019_lbp_multiclass.csv\"\n",
    "    GLCM_OUT = \"isic2019_glcm_multiclass.csv\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"DATASET must be 'HAM10000' or 'ISIC2019'\")\n",
    "\n",
    "# =====================================================\n",
    "# LBP CONFIG\n",
    "# =====================================================\n",
    "\n",
    "LBP_RADIUS = 2\n",
    "LBP_POINTS = 8 * LBP_RADIUS\n",
    "LBP_METHOD = \"uniform\"\n",
    "\n",
    "# =====================================================\n",
    "# GLCM CONFIG\n",
    "# =====================================================\n",
    "\n",
    "GLCM_DISTANCES = [1, 2]\n",
    "GLCM_ANGLES = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "GLCM_PROPS = [\"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\", \"correlation\"]\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def extract_lbp(gray):\n",
    "    lbp = local_binary_pattern(gray, LBP_POINTS, LBP_RADIUS, method=LBP_METHOD)\n",
    "    hist, _ = np.histogram(\n",
    "        lbp.ravel(),\n",
    "        bins=np.arange(0, LBP_POINTS + 3),\n",
    "        range=(0, LBP_POINTS + 2),\n",
    "        density=True\n",
    "    )\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "def extract_glcm(gray):\n",
    "    glcm = graycomatrix(\n",
    "        gray,\n",
    "        distances=GLCM_DISTANCES,\n",
    "        angles=GLCM_ANGLES,\n",
    "        levels=256,\n",
    "        symmetric=True,\n",
    "        normed=True\n",
    "    )\n",
    "\n",
    "    feats = []\n",
    "    for prop in GLCM_PROPS:\n",
    "        feats.extend(graycoprops(glcm, prop).ravel())\n",
    "\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "# =====================================================\n",
    "# LOAD LABELS\n",
    "# =====================================================\n",
    "\n",
    "labels_df = pd.read_csv(LABEL_CSV)\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE EXTRACTION\n",
    "# =====================================================\n",
    "\n",
    "lbp_rows = []\n",
    "glcm_rows = []\n",
    "\n",
    "for _, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"Extracting features\"):\n",
    "\n",
    "    # ---------- LABEL ----------\n",
    "    if DATASET == \"HAM10000\":\n",
    "        image_id = row[\"image_id\"]\n",
    "        label = CLASS_MAP[row[\"dx\"]]\n",
    "    else:\n",
    "        image_id = row[\"image\"]\n",
    "        label_name = max(CLASS_MAP, key=lambda c: row[c])\n",
    "        label = CLASS_MAP[label_name]\n",
    "\n",
    "    # ---------- IMAGE ----------\n",
    "    img_path = os.path.join(IMAGE_DIR, image_id + \".jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ---------- FEATURES ----------\n",
    "    lbp_feat = extract_lbp(gray)\n",
    "    glcm_feat = extract_glcm(gray)\n",
    "\n",
    "    lbp_rows.append(np.concatenate([lbp_feat, [label]]))\n",
    "    glcm_rows.append(np.concatenate([glcm_feat, [label]]))\n",
    "\n",
    "# =====================================================\n",
    "# SAVE LBP CSV\n",
    "# =====================================================\n",
    "\n",
    "lbp_feature_names = [f\"lbp_{i}\" for i in range(len(lbp_feat))] + [\"label\"]\n",
    "lbp_df = pd.DataFrame(lbp_rows, columns=lbp_feature_names)\n",
    "lbp_df.to_csv(LBP_OUT, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# SAVE GLCM CSV\n",
    "# =====================================================\n",
    "\n",
    "glcm_feature_names = [f\"glcm_{i}\" for i in range(len(glcm_feat))] + [\"label\"]\n",
    "glcm_df = pd.DataFrame(glcm_rows, columns=glcm_feature_names)\n",
    "glcm_df.to_csv(GLCM_OUT, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# DONE\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\nFeature extraction completed successfully.\")\n",
    "print(f\"LBP  file saved: {LBP_OUT}  | Shape: {lbp_df.shape}\")\n",
    "print(f\"GLCM file saved: {GLCM_OUT} | Shape: {glcm_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27b93b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "INPUT_CSV = \"isic2019_lbp_multiclass.csv\"\n",
    "OUTPUT_CSV = \"isic2019_lbp_multiclass_clean_norm.csv\"\n",
    "\n",
    "LABEL_COL = \"label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72f6c74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (25331, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbp_0</th>\n",
       "      <th>lbp_1</th>\n",
       "      <th>lbp_2</th>\n",
       "      <th>lbp_3</th>\n",
       "      <th>lbp_4</th>\n",
       "      <th>lbp_5</th>\n",
       "      <th>lbp_6</th>\n",
       "      <th>lbp_7</th>\n",
       "      <th>lbp_8</th>\n",
       "      <th>lbp_9</th>\n",
       "      <th>lbp_10</th>\n",
       "      <th>lbp_11</th>\n",
       "      <th>lbp_12</th>\n",
       "      <th>lbp_13</th>\n",
       "      <th>lbp_14</th>\n",
       "      <th>lbp_15</th>\n",
       "      <th>lbp_16</th>\n",
       "      <th>lbp_17</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027909</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.021639</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.020991</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>0.043783</td>\n",
       "      <td>0.057602</td>\n",
       "      <td>0.055127</td>\n",
       "      <td>0.094185</td>\n",
       "      <td>0.045787</td>\n",
       "      <td>0.080855</td>\n",
       "      <td>0.044272</td>\n",
       "      <td>0.040362</td>\n",
       "      <td>0.032825</td>\n",
       "      <td>0.022049</td>\n",
       "      <td>0.165044</td>\n",
       "      <td>0.175190</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022377</td>\n",
       "      <td>0.013428</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.020950</td>\n",
       "      <td>0.015851</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>0.048734</td>\n",
       "      <td>0.044876</td>\n",
       "      <td>0.102694</td>\n",
       "      <td>0.040068</td>\n",
       "      <td>0.091610</td>\n",
       "      <td>0.046321</td>\n",
       "      <td>0.043776</td>\n",
       "      <td>0.037458</td>\n",
       "      <td>0.023365</td>\n",
       "      <td>0.193819</td>\n",
       "      <td>0.166085</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029187</td>\n",
       "      <td>0.017542</td>\n",
       "      <td>0.023102</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.027765</td>\n",
       "      <td>0.040670</td>\n",
       "      <td>0.055784</td>\n",
       "      <td>0.079202</td>\n",
       "      <td>0.078902</td>\n",
       "      <td>0.097269</td>\n",
       "      <td>0.065076</td>\n",
       "      <td>0.069252</td>\n",
       "      <td>0.046294</td>\n",
       "      <td>0.040628</td>\n",
       "      <td>0.031325</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>0.075484</td>\n",
       "      <td>0.176301</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022043</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>0.016559</td>\n",
       "      <td>0.028292</td>\n",
       "      <td>0.040338</td>\n",
       "      <td>0.052436</td>\n",
       "      <td>0.053868</td>\n",
       "      <td>0.103538</td>\n",
       "      <td>0.039573</td>\n",
       "      <td>0.088584</td>\n",
       "      <td>0.040079</td>\n",
       "      <td>0.037639</td>\n",
       "      <td>0.032074</td>\n",
       "      <td>0.021151</td>\n",
       "      <td>0.225758</td>\n",
       "      <td>0.147524</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>0.012155</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>0.020676</td>\n",
       "      <td>0.029706</td>\n",
       "      <td>0.052718</td>\n",
       "      <td>0.073202</td>\n",
       "      <td>0.074809</td>\n",
       "      <td>0.032827</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.022023</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>0.431523</td>\n",
       "      <td>0.105613</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lbp_0     lbp_1     lbp_2     lbp_3     lbp_4     lbp_5     lbp_6  \\\n",
       "0  0.027909  0.016052  0.021639  0.023069  0.020991  0.033258  0.043783   \n",
       "1  0.022377  0.013428  0.022639  0.020950  0.015851  0.027826  0.038123   \n",
       "2  0.029187  0.017542  0.023102  0.027211  0.027765  0.040670  0.055784   \n",
       "3  0.022043  0.013344  0.018555  0.018645  0.016559  0.028292  0.040338   \n",
       "4  0.016508  0.009536  0.012155  0.013560  0.013298  0.020676  0.029706   \n",
       "\n",
       "      lbp_7     lbp_8     lbp_9    lbp_10    lbp_11    lbp_12    lbp_13  \\\n",
       "0  0.057602  0.055127  0.094185  0.045787  0.080855  0.044272  0.040362   \n",
       "1  0.048734  0.044876  0.102694  0.040068  0.091610  0.046321  0.043776   \n",
       "2  0.079202  0.078902  0.097269  0.065076  0.069252  0.046294  0.040628   \n",
       "3  0.052436  0.053868  0.103538  0.039573  0.088584  0.040079  0.037639   \n",
       "4  0.052718  0.073202  0.074809  0.032827  0.043240  0.022023  0.020369   \n",
       "\n",
       "     lbp_14    lbp_15    lbp_16    lbp_17  label  \n",
       "0  0.032825  0.022049  0.165044  0.175190    5.0  \n",
       "1  0.037458  0.023365  0.193819  0.166085    5.0  \n",
       "2  0.031325  0.019006  0.075484  0.176301    4.0  \n",
       "3  0.032074  0.021151  0.225758  0.147524    5.0  \n",
       "4  0.016868  0.011370  0.431523  0.105613    4.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "print(\"Initial shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20b3d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 18\n",
      "Label distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "5.0    12875\n",
       "4.0     4522\n",
       "1.0     3323\n",
       "2.0     2624\n",
       "0.0      867\n",
       "6.0      628\n",
       "7.0      253\n",
       "3.0      239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [c for c in df.columns if c != LABEL_COL]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(\"Label distribution:\")\n",
    "df[LABEL_COL].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bab9f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing NaN / Inf rows: (25331, 19)\n"
     ]
    }
   ],
   "source": [
    "# Replace infinite values with NaN\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Drop rows containing NaN\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "print(\"After removing NaN / Inf rows:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cc428f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant features detected: 0\n",
      "Remaining features: 18\n"
     ]
    }
   ],
   "source": [
    "constant_features = [c for c in feature_cols if df[c].nunique() <= 1]\n",
    "\n",
    "print(f\"Constant features detected: {len(constant_features)}\")\n",
    "\n",
    "df = df.drop(columns=constant_features)\n",
    "\n",
    "# Update feature list\n",
    "feature_cols = [c for c in feature_cols if c not in constant_features]\n",
    "\n",
    "print(\"Remaining features:\", len(feature_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65eb647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers clipped feature-wise using IQR\n"
     ]
    }
   ],
   "source": [
    "def clip_outliers_iqr(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return series.clip(lower, upper)\n",
    "\n",
    "# Apply per feature\n",
    "for col in feature_cols:\n",
    "    df[col] = clip_outliers_iqr(df[col])\n",
    "\n",
    "print(\"Outliers clipped feature-wise using IQR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcc8ab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-wise Min–Max normalization applied\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalize(series):\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if max_val > min_val:\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Normalize each feature independently\n",
    "for col in feature_cols:\n",
    "    df[col] = min_max_normalize(df[col])\n",
    "\n",
    "print(\"Feature-wise Min–Max normalization applied\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04781e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature value ranges (should be 0–1):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbp_0</th>\n",
       "      <th>lbp_1</th>\n",
       "      <th>lbp_2</th>\n",
       "      <th>lbp_3</th>\n",
       "      <th>lbp_4</th>\n",
       "      <th>lbp_5</th>\n",
       "      <th>lbp_6</th>\n",
       "      <th>lbp_7</th>\n",
       "      <th>lbp_8</th>\n",
       "      <th>lbp_9</th>\n",
       "      <th>lbp_10</th>\n",
       "      <th>lbp_11</th>\n",
       "      <th>lbp_12</th>\n",
       "      <th>lbp_13</th>\n",
       "      <th>lbp_14</th>\n",
       "      <th>lbp_15</th>\n",
       "      <th>lbp_16</th>\n",
       "      <th>lbp_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lbp_0  lbp_1  lbp_2  lbp_3  lbp_4  lbp_5  lbp_6  lbp_7  lbp_8  lbp_9  \\\n",
       "min    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "max    1.0    1.0    1.0    1.0    1.0    1.0    1.0    1.0    1.0    1.0   \n",
       "\n",
       "     lbp_10  lbp_11  lbp_12  lbp_13  lbp_14  lbp_15  lbp_16  lbp_17  \n",
       "min     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "max     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check range\n",
    "print(\"Feature value ranges (should be 0–1):\")\n",
    "df[feature_cols].describe().loc[[\"min\", \"max\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3af27107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned & normalized dataset:\n",
      "isic2019_lbp_multiclass_clean_norm.csv\n",
      "Final shape: (25331, 19)\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"Saved cleaned & normalized dataset:\")\n",
    "print(OUTPUT_CSV)\n",
    "print(\"Final shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459e758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d874b43f",
   "metadata": {},
   "source": [
    "**With Folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =====================================================\n",
    "# PATH SETUP\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"Code\", \"scikit-ExSTraCS-master\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from skExSTraCS.ExSTraCS import ExSTraCS\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# METRICS (binary-safe, multiclass-ready later)\n",
    "# =====================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return bal_acc, cm\n",
    "\n",
    "# =====================================================\n",
    "# CROSS-VALIDATION CORE\n",
    "# =====================================================\n",
    "\n",
    "def run_cv(csv_path, dataset_name, feature_family, param_grid,\n",
    "           n_splits=5, out_dir=\"lcs\"):\n",
    "\n",
    "    print(f\"\\n=== {dataset_name} | {feature_family} ===\")\n",
    "\n",
    "    csv_path = os.path.join(PROJECT_ROOT, csv_path)\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    feature_cols = [c for c in data.columns if c not in (\"image\", \"label\")]\n",
    "    X = data[feature_cols].values.astype(float)\n",
    "    y = data[\"label\"].values.astype(int)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_results = []\n",
    "    per_fold_records = []\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(\"Params:\", params)\n",
    "        fold_scores = []\n",
    "\n",
    "        for fold, (tr_idx, te_idx) in enumerate(skf.split(X, y), 1):\n",
    "            seed = 42 + fold\n",
    "\n",
    "            # =========================\n",
    "            # FOLD-WISE NORMALIZATION\n",
    "            # =========================\n",
    "            X_tr_raw, X_te_raw = X[tr_idx], X[te_idx]\n",
    "            y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_tr = scaler.fit_transform(X_tr_raw)\n",
    "            X_te = scaler.transform(X_te_raw)\n",
    "\n",
    "            # =========================\n",
    "            # MODEL SETUP\n",
    "            # =========================\n",
    "            model = ExSTraCS()\n",
    "            model.N = params.get(\"N\", 2000)\n",
    "            model.learningIterations = params.get(\"learningIterations\", 100000)\n",
    "            model.theta_sel = params.get(\"theta_sel\", 0.8)\n",
    "\n",
    "            # Minority & specificity bias (as before)\n",
    "            model.nu = params.get(\"nu\", 3.0)\n",
    "            model.p_spec = params.get(\"p_spec\", 0.4)\n",
    "            model.theta_GA = params.get(\"theta_GA\", 15)\n",
    "            model.chi = params.get(\"chi\", 0.8)\n",
    "            model.mu = params.get(\"mu\", 0.04)\n",
    "\n",
    "            model.doSubsumption = True\n",
    "            model.useBalancedAccuracy = True\n",
    "            model.randomSeed = seed\n",
    "\n",
    "            print(\n",
    "                f\"  Fold {fold} | seed={seed} \"\n",
    "                f\"N={model.N} iters={model.learningIterations}\"\n",
    "            )\n",
    "\n",
    "            start = time.time()\n",
    "            fit_exception = None\n",
    "\n",
    "            try:\n",
    "                model.fit(X_tr, y_tr)\n",
    "                y_pred = model.predict(X_te)\n",
    "                bal_acc, cm = compute_metrics(y_te, y_pred)\n",
    "            except Exception:\n",
    "                fit_exception = traceback.format_exc()\n",
    "                bal_acc = None\n",
    "                cm = None\n",
    "\n",
    "            duration = time.time() - start\n",
    "\n",
    "            try:\n",
    "                pop_size = len(model.population.popSet)\n",
    "            except Exception:\n",
    "                pop_size = None\n",
    "\n",
    "            print(\n",
    "                f\"    BA={bal_acc} | time={duration:.1f}s | rules={pop_size}\"\n",
    "            )\n",
    "\n",
    "            per_fold_records.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"feature_family\": feature_family,\n",
    "                \"params\": params,\n",
    "                \"fold\": fold,\n",
    "                \"balanced_accuracy\": bal_acc,\n",
    "                \"confusion_matrix\": cm.tolist() if cm is not None else None,\n",
    "                \"duration_seconds\": round(duration, 3),\n",
    "                \"rule_population\": pop_size,\n",
    "                \"fit_exception\": fit_exception\n",
    "            })\n",
    "\n",
    "            if bal_acc is not None:\n",
    "                fold_scores.append(bal_acc)\n",
    "\n",
    "        all_results.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"feature_family\": feature_family,\n",
    "            \"params\": params,\n",
    "            \"mean_bal_acc\": float(np.mean(fold_scores)) if fold_scores else None,\n",
    "            \"std_bal_acc\": float(np.std(fold_scores)) if fold_scores else None,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    # =========================\n",
    "    # SAVE RESULTS\n",
    "    # =========================\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    folds_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_folds_{ts}.jsonl\"\n",
    "    )\n",
    "    summary_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_summary_{ts}.json\"\n",
    "    )\n",
    "\n",
    "    with open(folds_path, \"w\") as fh:\n",
    "        for rec in per_fold_records:\n",
    "            fh.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    with open(summary_path, \"w\") as fh:\n",
    "        json.dump(all_results, fh, indent=2)\n",
    "\n",
    "    print(f\"Saved folds   -> {folds_path}\")\n",
    "    print(f\"Saved summary -> {summary_path}\")\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# =====================================================\n",
    "# MAIN: SEQUENTIAL EXPERIMENTS\n",
    "# =====================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = [\n",
    "        {\"N\": 1500, \"learningIterations\": 100000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.9},\n",
    "    ]\n",
    "\n",
    "    experiments = [\n",
    "        # HAM10000\n",
    "        (\"ham10000_lbp_multiclass.csv\",  \"HAM10000\", \"LBP\"),\n",
    "        (\"ham10000_glcm_multiclass.csv\", \"HAM10000\", \"GLCM\"),\n",
    "\n",
    "        # ISIC2019\n",
    "        (\"isic2019_lbp_multiclass.csv\",  \"ISIC2019\", \"LBP\"),\n",
    "        (\"isic2019_glcm_multiclass.csv\", \"ISIC2019\", \"GLCM\"),\n",
    "    ]\n",
    "\n",
    "    out_dir = os.path.join(PROJECT_ROOT, \"lcs\")\n",
    "\n",
    "    all_runs = []\n",
    "\n",
    "    for csv_path, dataset, feature_family in experiments:\n",
    "        df = run_cv(\n",
    "            csv_path=csv_path,\n",
    "            dataset_name=dataset,\n",
    "            feature_family=feature_family,\n",
    "            param_grid=param_grid,\n",
    "            n_splits=5,\n",
    "            out_dir=out_dir\n",
    "        )\n",
    "        all_runs.append(df)\n",
    "\n",
    "    final_df = pd.concat(all_runs, ignore_index=True)\n",
    "\n",
    "    final_out = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_feature_family_comparison_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    )\n",
    "\n",
    "    final_df.to_csv(final_out, index=False)\n",
    "    print(f\"\\nALL DONE. Final comparison CSV saved to:\\n{final_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9895ca",
   "metadata": {},
   "source": [
    "**Without Folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "# =====================================================\n",
    "# PATH SETUP\n",
    "# =====================================================\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"Code\", \"scikit-ExSTraCS-master\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from skExSTraCS.ExSTraCS import ExSTraCS\n",
    "\n",
    "# =====================================================\n",
    "# METRICS\n",
    "# =====================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return bal_acc, cm\n",
    "\n",
    "# =====================================================\n",
    "# CROSS-VALIDATION CORE\n",
    "# =====================================================\n",
    "\n",
    "def run_cv(csv_path, dataset_name, feature_family, param_grid,\n",
    "           n_splits=5, out_dir=\"lcs\"):\n",
    "\n",
    "    print(f\"\\n=== {dataset_name} | {feature_family} ===\")\n",
    "\n",
    "    csv_path = os.path.join(PROJECT_ROOT, csv_path)\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    feature_cols = [c for c in data.columns if c not in (\"image\", \"label\")]\n",
    "    X = data[feature_cols].values.astype(float)\n",
    "    y = data[\"label\"].values.astype(int)\n",
    "\n",
    "    # -------------------------\n",
    "    # SAFETY CHECK (OPTIONAL)\n",
    "    # -------------------------\n",
    "    if np.nanmin(X) < 0.0 or np.nanmax(X) > 1.0:\n",
    "        print(\"⚠ WARNING: Features not strictly in [0,1]. \"\n",
    "              \"Ensure preprocessing was applied correctly.\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_results = []\n",
    "    per_fold_records = []\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(\"Params:\", params)\n",
    "        fold_scores = []\n",
    "\n",
    "        for fold, (tr_idx, te_idx) in enumerate(skf.split(X, y), 1):\n",
    "            seed = 42 + fold\n",
    "\n",
    "            # =========================\n",
    "            # NO NORMALIZATION HERE\n",
    "            # =========================\n",
    "            X_tr = X[tr_idx]\n",
    "            X_te = X[te_idx]\n",
    "            y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "            # =========================\n",
    "            # MODEL SETUP\n",
    "            # =========================\n",
    "            model = ExSTraCS()\n",
    "            model.N = params.get(\"N\", 2000)\n",
    "            model.learningIterations = params.get(\"learningIterations\", 100000)\n",
    "            model.theta_sel = params.get(\"theta_sel\", 0.8)\n",
    "\n",
    "            # Minority & specificity bias\n",
    "            model.nu = params.get(\"nu\", 3.0)\n",
    "            model.p_spec = params.get(\"p_spec\", 0.4)\n",
    "            model.theta_GA = params.get(\"theta_GA\", 15)\n",
    "            model.chi = params.get(\"chi\", 0.8)\n",
    "            model.mu = params.get(\"mu\", 0.04)\n",
    "\n",
    "            model.doSubsumption = True\n",
    "            model.useBalancedAccuracy = True\n",
    "            model.randomSeed = seed\n",
    "\n",
    "            print(\n",
    "                f\"  Fold {fold} | seed={seed} \"\n",
    "                f\"N={model.N} iters={model.learningIterations}\"\n",
    "            )\n",
    "\n",
    "            start = time.time()\n",
    "            fit_exception = None\n",
    "\n",
    "            try:\n",
    "                model.fit(X_tr, y_tr)\n",
    "                y_pred = model.predict(X_te)\n",
    "                bal_acc, cm = compute_metrics(y_te, y_pred)\n",
    "            except Exception:\n",
    "                fit_exception = traceback.format_exc()\n",
    "                bal_acc = None\n",
    "                cm = None\n",
    "\n",
    "            duration = time.time() - start\n",
    "\n",
    "            try:\n",
    "                pop_size = len(model.population.popSet)\n",
    "            except Exception:\n",
    "                pop_size = None\n",
    "\n",
    "            print(\n",
    "                f\"    BA={bal_acc} | time={duration:.1f}s | rules={pop_size}\"\n",
    "            )\n",
    "\n",
    "            per_fold_records.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"feature_family\": feature_family,\n",
    "                \"params\": params,\n",
    "                \"fold\": fold,\n",
    "                \"balanced_accuracy\": bal_acc,\n",
    "                \"confusion_matrix\": cm.tolist() if cm is not None else None,\n",
    "                \"duration_seconds\": round(duration, 3),\n",
    "                \"rule_population\": pop_size,\n",
    "                \"fit_exception\": fit_exception\n",
    "            })\n",
    "\n",
    "            if bal_acc is not None:\n",
    "                fold_scores.append(bal_acc)\n",
    "\n",
    "        all_results.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"feature_family\": feature_family,\n",
    "            \"params\": params,\n",
    "            \"mean_bal_acc\": float(np.mean(fold_scores)) if fold_scores else None,\n",
    "            \"std_bal_acc\": float(np.std(fold_scores)) if fold_scores else None,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    # =========================\n",
    "    # SAVE RESULTS\n",
    "    # =========================\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    folds_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_folds_{ts}.jsonl\"\n",
    "    )\n",
    "    summary_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_summary_{ts}.json\"\n",
    "    )\n",
    "\n",
    "    with open(folds_path, \"w\") as fh:\n",
    "        for rec in per_fold_records:\n",
    "            fh.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    with open(summary_path, \"w\") as fh:\n",
    "        json.dump(all_results, fh, indent=2)\n",
    "\n",
    "    print(f\"Saved folds   -> {folds_path}\")\n",
    "    print(f\"Saved summary -> {summary_path}\")\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# =====================================================\n",
    "# MAIN\n",
    "# =====================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = [\n",
    "        {\"N\": 1500, \"learningIterations\": 100000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.9},\n",
    "    ]\n",
    "\n",
    "    experiments = [\n",
    "        (\"csv_outputs/ham10000_lbp_multiclass_clean_norm.csv\",  \"HAM10000\", \"LBP\"),\n",
    "        (\"csv_outputs/ham10000_glcm_multiclass_clean_norm.csv\", \"HAM10000\", \"GLCM\"),\n",
    "        (\"csv_outputs/isic2019_lbp_multiclass_clean_norm.csv\",  \"ISIC2019\", \"LBP\"),\n",
    "        (\"csv_outputs/isic2019_glcm_multiclass_clean_norm.csv\", \"ISIC2019\", \"GLCM\"),\n",
    "    ]\n",
    "\n",
    "    out_dir = os.path.join(PROJECT_ROOT, \"lcs\")\n",
    "\n",
    "    all_runs = []\n",
    "\n",
    "    for csv_path, dataset, feature_family in experiments:\n",
    "        df = run_cv(\n",
    "            csv_path=csv_path,\n",
    "            dataset_name=dataset,\n",
    "            feature_family=feature_family,\n",
    "            param_grid=param_grid,\n",
    "            n_splits=5,\n",
    "            out_dir=out_dir\n",
    "        )\n",
    "        all_runs.append(df)\n",
    "\n",
    "    final_df = pd.concat(all_runs, ignore_index=True)\n",
    "\n",
    "    final_out = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_feature_family_comparison_cleanNorm_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    )\n",
    "\n",
    "    final_df.to_csv(final_out, index=False)\n",
    "    print(f\"\\nALL DONE. Final comparison CSV saved to:\\n{final_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
