{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1286742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 10015/10015 [40:27<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction completed successfully.\n",
      "LBP  file saved: ham10000_lbp_multiclass.csv  | Shape: (10015, 19)\n",
      "GLCM file saved: ham10000_glcm_multiclass.csv | Shape: (10015, 41)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "\n",
    "# =====================================================\n",
    "# CONFIG\n",
    "# =====================================================\n",
    "\n",
    "DATASET = \"HAM10000\"   # \"HAM10000\" or \"ISIC2019\"\n",
    "CLEAN_DATA_ROOT = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Code\\CleanData\"\n",
    "\n",
    "# =====================================================\n",
    "# DATASET PATHS & CLASS MAPS\n",
    "# =====================================================\n",
    "\n",
    "if DATASET == \"HAM10000\":\n",
    "    IMAGE_DIR = os.path.join(CLEAN_DATA_ROOT, \"HAM10000\", \"images\")\n",
    "    LABEL_CSV = os.path.join(CLEAN_DATA_ROOT, \"HAM10000\", \"HAM10000_metadata\")\n",
    "\n",
    "    CLASS_MAP = {\n",
    "        \"akiec\": 0,\n",
    "        \"bcc\": 1,\n",
    "        \"bkl\": 2,\n",
    "        \"df\": 3,\n",
    "        \"mel\": 4,\n",
    "        \"nv\": 5,\n",
    "        \"vasc\": 6\n",
    "    }\n",
    "\n",
    "    LBP_OUT = \"ham10000_lbp_multiclass.csv\"\n",
    "    GLCM_OUT = \"ham10000_glcm_multiclass.csv\"\n",
    "\n",
    "elif DATASET == \"ISIC2019\":\n",
    "    IMAGE_DIR = os.path.join(CLEAN_DATA_ROOT, \"ISIC2019\", \"images_train\")\n",
    "    LABEL_CSV = os.path.join(CLEAN_DATA_ROOT, \"ISIC2019\", \"ISIC_2019_Training_GroundTruth.csv\")\n",
    "\n",
    "    CLASS_MAP = {\n",
    "        \"AK\": 0,\n",
    "        \"BCC\": 1,\n",
    "        \"BKL\": 2,\n",
    "        \"DF\": 3,\n",
    "        \"MEL\": 4,\n",
    "        \"NV\": 5,\n",
    "        \"SCC\": 6,\n",
    "        \"VASC\": 7\n",
    "    }\n",
    "\n",
    "    LBP_OUT = \"isic2019_lbp_multiclass.csv\"\n",
    "    GLCM_OUT = \"isic2019_glcm_multiclass.csv\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"DATASET must be 'HAM10000' or 'ISIC2019'\")\n",
    "\n",
    "# =====================================================\n",
    "# LBP CONFIG\n",
    "# =====================================================\n",
    "\n",
    "LBP_RADIUS = 2\n",
    "LBP_POINTS = 8 * LBP_RADIUS\n",
    "LBP_METHOD = \"uniform\"\n",
    "\n",
    "# =====================================================\n",
    "# GLCM CONFIG\n",
    "# =====================================================\n",
    "\n",
    "GLCM_DISTANCES = [1, 2]\n",
    "GLCM_ANGLES = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "GLCM_PROPS = [\"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\", \"correlation\"]\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def extract_lbp(gray):\n",
    "    lbp = local_binary_pattern(gray, LBP_POINTS, LBP_RADIUS, method=LBP_METHOD)\n",
    "    hist, _ = np.histogram(\n",
    "        lbp.ravel(),\n",
    "        bins=np.arange(0, LBP_POINTS + 3),\n",
    "        range=(0, LBP_POINTS + 2),\n",
    "        density=True\n",
    "    )\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "def extract_glcm(gray):\n",
    "    glcm = graycomatrix(\n",
    "        gray,\n",
    "        distances=GLCM_DISTANCES,\n",
    "        angles=GLCM_ANGLES,\n",
    "        levels=256,\n",
    "        symmetric=True,\n",
    "        normed=True\n",
    "    )\n",
    "\n",
    "    feats = []\n",
    "    for prop in GLCM_PROPS:\n",
    "        feats.extend(graycoprops(glcm, prop).ravel())\n",
    "\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "# =====================================================\n",
    "# LOAD LABELS\n",
    "# =====================================================\n",
    "\n",
    "labels_df = pd.read_csv(LABEL_CSV)\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE EXTRACTION\n",
    "# =====================================================\n",
    "\n",
    "lbp_rows = []\n",
    "glcm_rows = []\n",
    "\n",
    "for _, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"Extracting features\"):\n",
    "\n",
    "    # ---------- LABEL ----------\n",
    "    if DATASET == \"HAM10000\":\n",
    "        image_id = row[\"image_id\"]\n",
    "        label = CLASS_MAP[row[\"dx\"]]\n",
    "    else:\n",
    "        image_id = row[\"image\"]\n",
    "        label_name = max(CLASS_MAP, key=lambda c: row[c])\n",
    "        label = CLASS_MAP[label_name]\n",
    "\n",
    "    # ---------- IMAGE ----------\n",
    "    img_path = os.path.join(IMAGE_DIR, image_id + \".jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ---------- FEATURES ----------\n",
    "    lbp_feat = extract_lbp(gray)\n",
    "    glcm_feat = extract_glcm(gray)\n",
    "\n",
    "    lbp_rows.append(np.concatenate([lbp_feat, [label]]))\n",
    "    glcm_rows.append(np.concatenate([glcm_feat, [label]]))\n",
    "\n",
    "# =====================================================\n",
    "# SAVE LBP CSV\n",
    "# =====================================================\n",
    "\n",
    "lbp_feature_names = [f\"lbp_{i}\" for i in range(len(lbp_feat))] + [\"label\"]\n",
    "lbp_df = pd.DataFrame(lbp_rows, columns=lbp_feature_names)\n",
    "lbp_df.to_csv(LBP_OUT, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# SAVE GLCM CSV\n",
    "# =====================================================\n",
    "\n",
    "glcm_feature_names = [f\"glcm_{i}\" for i in range(len(glcm_feat))] + [\"label\"]\n",
    "glcm_df = pd.DataFrame(glcm_rows, columns=glcm_feature_names)\n",
    "glcm_df.to_csv(GLCM_OUT, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# DONE\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\nFeature extraction completed successfully.\")\n",
    "print(f\"LBP  file saved: {LBP_OUT}  | Shape: {lbp_df.shape}\")\n",
    "print(f\"GLCM file saved: {GLCM_OUT} | Shape: {glcm_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ecd6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 25331/25331 [2:17:26<00:00,  3.07it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction completed successfully.\n",
      "LBP  file saved: isic2019_lbp_multiclass.csv  | Shape: (25331, 19)\n",
      "GLCM file saved: isic2019_glcm_multiclass.csv | Shape: (25331, 41)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "\n",
    "# =====================================================\n",
    "# CONFIG\n",
    "# =====================================================\n",
    "\n",
    "DATASET = \"ISIC2019\"   # \"HAM10000\" or \"ISIC2019\"\n",
    "CLEAN_DATA_ROOT = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Code\\CleanData\"\n",
    "\n",
    "# =====================================================\n",
    "# DATASET PATHS & CLASS MAPS\n",
    "# =====================================================\n",
    "\n",
    "if DATASET == \"HAM10000\":\n",
    "    IMAGE_DIR = os.path.join(CLEAN_DATA_ROOT, \"HAM10000\", \"images\")\n",
    "    LABEL_CSV = os.path.join(CLEAN_DATA_ROOT, \"HAM10000\", \"HAM10000_metadata\")\n",
    "\n",
    "    CLASS_MAP = {\n",
    "        \"akiec\": 0,\n",
    "        \"bcc\": 1,\n",
    "        \"bkl\": 2,\n",
    "        \"df\": 3,\n",
    "        \"mel\": 4,\n",
    "        \"nv\": 5,\n",
    "        \"vasc\": 6\n",
    "    }\n",
    "\n",
    "    LBP_OUT = \"ham10000_lbp_multiclass.csv\"\n",
    "    GLCM_OUT = \"ham10000_glcm_multiclass.csv\"\n",
    "\n",
    "elif DATASET == \"ISIC2019\":\n",
    "    IMAGE_DIR = os.path.join(CLEAN_DATA_ROOT, \"ISIC2019\", \"images_train\")\n",
    "    LABEL_CSV = os.path.join(CLEAN_DATA_ROOT, \"ISIC2019\", \"ISIC_2019_Training_GroundTruth.csv\")\n",
    "\n",
    "    CLASS_MAP = {\n",
    "        \"AK\": 0,\n",
    "        \"BCC\": 1,\n",
    "        \"BKL\": 2,\n",
    "        \"DF\": 3,\n",
    "        \"MEL\": 4,\n",
    "        \"NV\": 5,\n",
    "        \"SCC\": 6,\n",
    "        \"VASC\": 7\n",
    "    }\n",
    "\n",
    "    LBP_OUT = \"isic2019_lbp_multiclass.csv\"\n",
    "    GLCM_OUT = \"isic2019_glcm_multiclass.csv\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"DATASET must be 'HAM10000' or 'ISIC2019'\")\n",
    "\n",
    "# =====================================================\n",
    "# LBP CONFIG\n",
    "# =====================================================\n",
    "\n",
    "LBP_RADIUS = 2\n",
    "LBP_POINTS = 8 * LBP_RADIUS\n",
    "LBP_METHOD = \"uniform\"\n",
    "\n",
    "# =====================================================\n",
    "# GLCM CONFIG\n",
    "# =====================================================\n",
    "\n",
    "GLCM_DISTANCES = [1, 2]\n",
    "GLCM_ANGLES = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "GLCM_PROPS = [\"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\", \"correlation\"]\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def extract_lbp(gray):\n",
    "    lbp = local_binary_pattern(gray, LBP_POINTS, LBP_RADIUS, method=LBP_METHOD)\n",
    "    hist, _ = np.histogram(\n",
    "        lbp.ravel(),\n",
    "        bins=np.arange(0, LBP_POINTS + 3),\n",
    "        range=(0, LBP_POINTS + 2),\n",
    "        density=True\n",
    "    )\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "def extract_glcm(gray):\n",
    "    glcm = graycomatrix(\n",
    "        gray,\n",
    "        distances=GLCM_DISTANCES,\n",
    "        angles=GLCM_ANGLES,\n",
    "        levels=256,\n",
    "        symmetric=True,\n",
    "        normed=True\n",
    "    )\n",
    "\n",
    "    feats = []\n",
    "    for prop in GLCM_PROPS:\n",
    "        feats.extend(graycoprops(glcm, prop).ravel())\n",
    "\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "# =====================================================\n",
    "# LOAD LABELS\n",
    "# =====================================================\n",
    "\n",
    "labels_df = pd.read_csv(LABEL_CSV)\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE EXTRACTION\n",
    "# =====================================================\n",
    "\n",
    "lbp_rows = []\n",
    "glcm_rows = []\n",
    "\n",
    "for _, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"Extracting features\"):\n",
    "\n",
    "    # ---------- LABEL ----------\n",
    "    if DATASET == \"HAM10000\":\n",
    "        image_id = row[\"image_id\"]\n",
    "        label = CLASS_MAP[row[\"dx\"]]\n",
    "    else:\n",
    "        image_id = row[\"image\"]\n",
    "        label_name = max(CLASS_MAP, key=lambda c: row[c])\n",
    "        label = CLASS_MAP[label_name]\n",
    "\n",
    "    # ---------- IMAGE ----------\n",
    "    img_path = os.path.join(IMAGE_DIR, image_id + \".jpg\")\n",
    "    if not os.path.exists(img_path):\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ---------- FEATURES ----------\n",
    "    lbp_feat = extract_lbp(gray)\n",
    "    glcm_feat = extract_glcm(gray)\n",
    "\n",
    "    lbp_rows.append(np.concatenate([lbp_feat, [label]]))\n",
    "    glcm_rows.append(np.concatenate([glcm_feat, [label]]))\n",
    "\n",
    "# =====================================================\n",
    "# SAVE LBP CSV\n",
    "# =====================================================\n",
    "\n",
    "lbp_feature_names = [f\"lbp_{i}\" for i in range(len(lbp_feat))] + [\"label\"]\n",
    "lbp_df = pd.DataFrame(lbp_rows, columns=lbp_feature_names)\n",
    "lbp_df.to_csv(LBP_OUT, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# SAVE GLCM CSV\n",
    "# =====================================================\n",
    "\n",
    "glcm_feature_names = [f\"glcm_{i}\" for i in range(len(glcm_feat))] + [\"label\"]\n",
    "glcm_df = pd.DataFrame(glcm_rows, columns=glcm_feature_names)\n",
    "glcm_df.to_csv(GLCM_OUT, index=False)\n",
    "\n",
    "# =====================================================\n",
    "# DONE\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\nFeature extraction completed successfully.\")\n",
    "print(f\"LBP  file saved: {LBP_OUT}  | Shape: {lbp_df.shape}\")\n",
    "print(f\"GLCM file saved: {GLCM_OUT} | Shape: {glcm_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27b93b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "INPUT_CSV = \"isic2019_lbp_multiclass.csv\"\n",
    "OUTPUT_CSV = \"isic2019_lbp_multiclass_clean_norm.csv\"\n",
    "\n",
    "LABEL_COL = \"label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72f6c74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (25331, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbp_0</th>\n",
       "      <th>lbp_1</th>\n",
       "      <th>lbp_2</th>\n",
       "      <th>lbp_3</th>\n",
       "      <th>lbp_4</th>\n",
       "      <th>lbp_5</th>\n",
       "      <th>lbp_6</th>\n",
       "      <th>lbp_7</th>\n",
       "      <th>lbp_8</th>\n",
       "      <th>lbp_9</th>\n",
       "      <th>lbp_10</th>\n",
       "      <th>lbp_11</th>\n",
       "      <th>lbp_12</th>\n",
       "      <th>lbp_13</th>\n",
       "      <th>lbp_14</th>\n",
       "      <th>lbp_15</th>\n",
       "      <th>lbp_16</th>\n",
       "      <th>lbp_17</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027909</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.021639</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.020991</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>0.043783</td>\n",
       "      <td>0.057602</td>\n",
       "      <td>0.055127</td>\n",
       "      <td>0.094185</td>\n",
       "      <td>0.045787</td>\n",
       "      <td>0.080855</td>\n",
       "      <td>0.044272</td>\n",
       "      <td>0.040362</td>\n",
       "      <td>0.032825</td>\n",
       "      <td>0.022049</td>\n",
       "      <td>0.165044</td>\n",
       "      <td>0.175190</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022377</td>\n",
       "      <td>0.013428</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.020950</td>\n",
       "      <td>0.015851</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>0.048734</td>\n",
       "      <td>0.044876</td>\n",
       "      <td>0.102694</td>\n",
       "      <td>0.040068</td>\n",
       "      <td>0.091610</td>\n",
       "      <td>0.046321</td>\n",
       "      <td>0.043776</td>\n",
       "      <td>0.037458</td>\n",
       "      <td>0.023365</td>\n",
       "      <td>0.193819</td>\n",
       "      <td>0.166085</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029187</td>\n",
       "      <td>0.017542</td>\n",
       "      <td>0.023102</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.027765</td>\n",
       "      <td>0.040670</td>\n",
       "      <td>0.055784</td>\n",
       "      <td>0.079202</td>\n",
       "      <td>0.078902</td>\n",
       "      <td>0.097269</td>\n",
       "      <td>0.065076</td>\n",
       "      <td>0.069252</td>\n",
       "      <td>0.046294</td>\n",
       "      <td>0.040628</td>\n",
       "      <td>0.031325</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>0.075484</td>\n",
       "      <td>0.176301</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022043</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>0.016559</td>\n",
       "      <td>0.028292</td>\n",
       "      <td>0.040338</td>\n",
       "      <td>0.052436</td>\n",
       "      <td>0.053868</td>\n",
       "      <td>0.103538</td>\n",
       "      <td>0.039573</td>\n",
       "      <td>0.088584</td>\n",
       "      <td>0.040079</td>\n",
       "      <td>0.037639</td>\n",
       "      <td>0.032074</td>\n",
       "      <td>0.021151</td>\n",
       "      <td>0.225758</td>\n",
       "      <td>0.147524</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>0.012155</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>0.020676</td>\n",
       "      <td>0.029706</td>\n",
       "      <td>0.052718</td>\n",
       "      <td>0.073202</td>\n",
       "      <td>0.074809</td>\n",
       "      <td>0.032827</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.022023</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>0.431523</td>\n",
       "      <td>0.105613</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lbp_0     lbp_1     lbp_2     lbp_3     lbp_4     lbp_5     lbp_6  \\\n",
       "0  0.027909  0.016052  0.021639  0.023069  0.020991  0.033258  0.043783   \n",
       "1  0.022377  0.013428  0.022639  0.020950  0.015851  0.027826  0.038123   \n",
       "2  0.029187  0.017542  0.023102  0.027211  0.027765  0.040670  0.055784   \n",
       "3  0.022043  0.013344  0.018555  0.018645  0.016559  0.028292  0.040338   \n",
       "4  0.016508  0.009536  0.012155  0.013560  0.013298  0.020676  0.029706   \n",
       "\n",
       "      lbp_7     lbp_8     lbp_9    lbp_10    lbp_11    lbp_12    lbp_13  \\\n",
       "0  0.057602  0.055127  0.094185  0.045787  0.080855  0.044272  0.040362   \n",
       "1  0.048734  0.044876  0.102694  0.040068  0.091610  0.046321  0.043776   \n",
       "2  0.079202  0.078902  0.097269  0.065076  0.069252  0.046294  0.040628   \n",
       "3  0.052436  0.053868  0.103538  0.039573  0.088584  0.040079  0.037639   \n",
       "4  0.052718  0.073202  0.074809  0.032827  0.043240  0.022023  0.020369   \n",
       "\n",
       "     lbp_14    lbp_15    lbp_16    lbp_17  label  \n",
       "0  0.032825  0.022049  0.165044  0.175190    5.0  \n",
       "1  0.037458  0.023365  0.193819  0.166085    5.0  \n",
       "2  0.031325  0.019006  0.075484  0.176301    4.0  \n",
       "3  0.032074  0.021151  0.225758  0.147524    5.0  \n",
       "4  0.016868  0.011370  0.431523  0.105613    4.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "print(\"Initial shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20b3d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 18\n",
      "Label distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "5.0    12875\n",
       "4.0     4522\n",
       "1.0     3323\n",
       "2.0     2624\n",
       "0.0      867\n",
       "6.0      628\n",
       "7.0      253\n",
       "3.0      239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [c for c in df.columns if c != LABEL_COL]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(\"Label distribution:\")\n",
    "df[LABEL_COL].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bab9f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing NaN / Inf rows: (25331, 19)\n"
     ]
    }
   ],
   "source": [
    "# Replace infinite values with NaN\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Drop rows containing NaN\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "print(\"After removing NaN / Inf rows:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cc428f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant features detected: 0\n",
      "Remaining features: 18\n"
     ]
    }
   ],
   "source": [
    "constant_features = [c for c in feature_cols if df[c].nunique() <= 1]\n",
    "\n",
    "print(f\"Constant features detected: {len(constant_features)}\")\n",
    "\n",
    "df = df.drop(columns=constant_features)\n",
    "\n",
    "# Update feature list\n",
    "feature_cols = [c for c in feature_cols if c not in constant_features]\n",
    "\n",
    "print(\"Remaining features:\", len(feature_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcafa0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAM10000 Class Distribution:\n",
      "\n",
      "dx\n",
      "akiec     327\n",
      "bcc       514\n",
      "bkl      1099\n",
      "df        115\n",
      "mel      1113\n",
      "nv       6705\n",
      "vasc      142\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total images: 10015\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# PATH\n",
    "# =========================\n",
    "HAM_CSV = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Code\\CleanData\\HAM10000\\HAM10000_metadata\"\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "df = pd.read_csv(HAM_CSV)\n",
    "\n",
    "# =========================\n",
    "# COUNT CLASSES\n",
    "# =========================\n",
    "class_counts = df[\"dx\"].value_counts().sort_index()\n",
    "\n",
    "print(\"HAM10000 Class Distribution:\\n\")\n",
    "print(class_counts)\n",
    "\n",
    "print(\"\\nTotal images:\", class_counts.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "418e12eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC2019 Class Distribution:\n",
      "\n",
      "AK        867\n",
      "BCC      3323\n",
      "BKL      2624\n",
      "DF        239\n",
      "MEL      4522\n",
      "NV      12875\n",
      "SCC       628\n",
      "VASC      253\n",
      "dtype: int64\n",
      "\n",
      "Total images: 25331\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# PATH\n",
    "# =========================\n",
    "ISIC_CSV = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Code\\CleanData\\ISIC2019\\ISIC_2019_Training_GroundTruth.csv\"\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "df = pd.read_csv(ISIC_CSV)\n",
    "\n",
    "# =========================\n",
    "# CLASS NAMES\n",
    "# =========================\n",
    "classes = [\"AK\", \"BCC\", \"BKL\", \"DF\", \"MEL\", \"NV\", \"SCC\", \"VASC\"]\n",
    "\n",
    "# =========================\n",
    "# COUNT PER CLASS\n",
    "# =========================\n",
    "class_counts = df[classes].sum().astype(int)\n",
    "\n",
    "print(\"ISIC2019 Class Distribution:\\n\")\n",
    "print(class_counts)\n",
    "\n",
    "print(\"\\nTotal images:\", class_counts.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "156da57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       HAM10000\n",
      "dx             \n",
      "akiec       327\n",
      "bcc         514\n",
      "bkl        1099\n",
      "df          115\n",
      "mel        1113\n",
      "nv         6705\n",
      "vasc        142\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(HAM_CSV)\n",
    "class_counts = df[\"dx\"].value_counts()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"HAM10000\": class_counts.reindex(\n",
    "        [\"akiec\",\"bcc\",\"bkl\",\"df\",\"mel\",\"nv\",\"vasc\"]\n",
    "    )\n",
    "})\n",
    "\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ff4562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ISIC2019\n",
      "AK         867\n",
      "BCC       3323\n",
      "BKL       2624\n",
      "DF         239\n",
      "MEL       4522\n",
      "NV       12875\n",
      "SCC        628\n",
      "VASC       253\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"ISIC2019\": class_counts.reindex(\n",
    "        [\"AK\", \"BCC\", \"BKL\", \"DF\", \"MEL\", \"NV\", \"SCC\", \"VASC\"]\n",
    "    )\n",
    "})\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65eb647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers clipped feature-wise using IQR\n",
      "Used feature columns: ['age'] \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clip_outliers_iqr(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return series.clip(lower, upper)\n",
    "\n",
    "# Example: load data\n",
    "# df = pd.read_csv(\"your_file.csv\")\n",
    "\n",
    "# Pick only numeric feature columns, exclude known non-features\n",
    "exclude_cols = {\"image\", \"label\", \"dx\"}\n",
    "feature_cols = [\n",
    "    c for c in df.columns\n",
    "    if c not in exclude_cols and pd.api.types.is_numeric_dtype(df[c])\n",
    "]\n",
    "\n",
    "# Clip per feature\n",
    "for col in feature_cols:\n",
    "    df[col] = clip_outliers_iqr(df[col])\n",
    "\n",
    "print(\"Outliers clipped feature-wise using IQR\")\n",
    "print(\"Used feature columns:\", feature_cols[:10], \"...\" if len(feature_cols) > 10 else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcc8ab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-wise Min–Max normalization applied\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalize(series):\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if max_val > min_val:\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Normalize each feature independently\n",
    "for col in feature_cols:\n",
    "    df[col] = min_max_normalize(df[col])\n",
    "\n",
    "print(\"Feature-wise Min–Max normalization applied\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04781e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature value ranges (should be 0–1):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbp_0</th>\n",
       "      <th>lbp_1</th>\n",
       "      <th>lbp_2</th>\n",
       "      <th>lbp_3</th>\n",
       "      <th>lbp_4</th>\n",
       "      <th>lbp_5</th>\n",
       "      <th>lbp_6</th>\n",
       "      <th>lbp_7</th>\n",
       "      <th>lbp_8</th>\n",
       "      <th>lbp_9</th>\n",
       "      <th>lbp_10</th>\n",
       "      <th>lbp_11</th>\n",
       "      <th>lbp_12</th>\n",
       "      <th>lbp_13</th>\n",
       "      <th>lbp_14</th>\n",
       "      <th>lbp_15</th>\n",
       "      <th>lbp_16</th>\n",
       "      <th>lbp_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lbp_0  lbp_1  lbp_2  lbp_3  lbp_4  lbp_5  lbp_6  lbp_7  lbp_8  lbp_9  \\\n",
       "min    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "max    1.0    1.0    1.0    1.0    1.0    1.0    1.0    1.0    1.0    1.0   \n",
       "\n",
       "     lbp_10  lbp_11  lbp_12  lbp_13  lbp_14  lbp_15  lbp_16  lbp_17  \n",
       "min     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "max     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check range\n",
    "print(\"Feature value ranges (should be 0–1):\")\n",
    "df[feature_cols].describe().loc[[\"min\", \"max\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3af27107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned & normalized dataset:\n",
      "isic2019_lbp_multiclass_clean_norm.csv\n",
      "Final shape: (25331, 19)\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"Saved cleaned & normalized dataset:\")\n",
    "print(OUTPUT_CSV)\n",
    "print(\"Final shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459e758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d874b43f",
   "metadata": {},
   "source": [
    "**With Folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02ee8051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATASET: HAM10000 | FEATURES: LBP ===\n",
      "Feature range: min=0.0068, max=0.4853 (raw / un-normalized)\n",
      "Params: {'N': 1500, 'learningIterations': 100000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=1500 iters=100000\n",
      "    BA=0.14714621694487467 | time=980.5s | rules=1088\n",
      "  Fold 2 | seed=44 N=1500 iters=100000\n",
      "    BA=0.15154182134047905 | time=1028.0s | rules=1094\n",
      "  Fold 3 | seed=45 N=1500 iters=100000\n",
      "    BA=0.14714621694487467 | time=764.5s | rules=1106\n",
      "  Fold 4 | seed=46 N=1500 iters=100000\n",
      "    BA=0.15151515151515152 | time=1063.2s | rules=1039\n",
      "  Fold 5 | seed=47 N=1500 iters=100000\n",
      "    BA=0.16006662986528758 | time=1055.1s | rules=1047\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.14505494505494504 | time=1184.4s | rules=1416\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.15207590602299725 | time=883.7s | rules=1467\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.14934401914267686 | time=884.5s | rules=1425\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.15151515151515152 | time=865.2s | rules=1457\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.14782676217653795 | time=853.7s | rules=1484\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.9}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.14725274725274723 | time=851.4s | rules=1470\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.1470396866370021 | time=835.5s | rules=1457\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.15733095733095734 | time=840.6s | rules=1448\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.14718614718614717 | time=857.8s | rules=1399\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.15573762553628326 | time=848.9s | rules=1429\n",
      "Saved folds   -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_HAM10000_LBP_raw_folds_20260112_193227.jsonl\n",
      "Saved summary -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_HAM10000_LBP_raw_summary_20260112_193227.json\n",
      "\n",
      "=== DATASET: HAM10000 | FEATURES: GLCM ===\n",
      "Feature range: min=0.0082, max=1826.3015 (raw / un-normalized)\n",
      "Params: {'N': 1500, 'learningIterations': 100000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=1500 iters=100000\n",
      "    BA=0.14424410540915394 | time=577.5s | rules=1057\n",
      "  Fold 2 | seed=44 N=1500 iters=100000\n",
      "    BA=0.14339122753966102 | time=579.1s | rules=1042\n",
      "  Fold 3 | seed=45 N=1500 iters=100000\n",
      "    BA=0.14424410540915394 | time=585.4s | rules=1001\n",
      "  Fold 4 | seed=46 N=1500 iters=100000\n",
      "    BA=0.14425770308123248 | time=577.8s | rules=1063\n",
      "  Fold 5 | seed=47 N=1500 iters=100000\n",
      "    BA=0.14275061254927027 | time=590.9s | rules=1025\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.14424410540915394 | time=770.6s | rules=1357\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.14264408224139768 | time=799.7s | rules=1373\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.14424410540915394 | time=887.8s | rules=1434\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.14425770308123248 | time=861.4s | rules=1407\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.1441375751012814 | time=804.5s | rules=1346\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.9}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.14275061254927027 | time=808.8s | rules=1387\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.14275061254927027 | time=786.4s | rules=1394\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.14424410540915394 | time=799.2s | rules=1385\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.14415117277335993 | time=818.7s | rules=1376\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.1441375751012814 | time=808.6s | rules=1364\n",
      "Saved folds   -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_HAM10000_GLCM_raw_folds_20260112_223644.jsonl\n",
      "Saved summary -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_HAM10000_GLCM_raw_summary_20260112_223644.json\n",
      "\n",
      "=== DATASET: ISIC2019 | FEATURES: LBP ===\n",
      "Feature range: min=0.0028, max=0.6180 (raw / un-normalized)\n",
      "Params: {'N': 1500, 'learningIterations': 100000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=1500 iters=100000\n",
      "    BA=0.13086064474601727 | time=607.2s | rules=774\n",
      "  Fold 2 | seed=44 N=1500 iters=100000\n",
      "    BA=0.1253658209515636 | time=614.9s | rules=787\n",
      "  Fold 3 | seed=45 N=1500 iters=100000\n",
      "    BA=0.12833580390813187 | time=626.2s | rules=729\n",
      "  Fold 4 | seed=46 N=1500 iters=100000\n",
      "    BA=0.1284294942399387 | time=610.7s | rules=807\n",
      "  Fold 5 | seed=47 N=1500 iters=100000\n",
      "    BA=0.12573255863905833 | time=611.5s | rules=686\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.13019363029684386 | time=857.6s | rules=1027\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.1287734914878093 | time=848.0s | rules=1036\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.13089373905968404 | time=844.6s | rules=1063\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.12898719918190527 | time=969.5s | rules=1043\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.13131047854407046 | time=1110.1s | rules=987\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.9}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.1311656998796944 | time=1281.0s | rules=1003\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.13289674481329275 | time=1065.6s | rules=994\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.1295847596934693 | time=1027.8s | rules=995\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.12727269310078185 | time=937.6s | rules=957\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.13407323632239806 | time=936.2s | rules=943\n",
      "Saved folds   -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_ISIC2019_LBP_raw_folds_20260113_021234.jsonl\n",
      "Saved summary -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_ISIC2019_LBP_raw_summary_20260113_021234.json\n",
      "\n",
      "=== DATASET: ISIC2019 | FEATURES: GLCM ===\n",
      "Feature range: min=0.0082, max=1826.3015 (raw / un-normalized)\n",
      "Params: {'N': 1500, 'learningIterations': 100000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=1500 iters=100000\n",
      "    BA=0.12802365499114948 | time=619.8s | rules=717\n",
      "  Fold 2 | seed=44 N=1500 iters=100000\n",
      "    BA=0.12997652830384995 | time=633.9s | rules=682\n",
      "  Fold 3 | seed=45 N=1500 iters=100000\n",
      "    BA=0.1278331471775926 | time=620.6s | rules=777\n",
      "  Fold 4 | seed=46 N=1500 iters=100000\n",
      "    BA=0.12794935131884183 | time=640.8s | rules=722\n",
      "  Fold 5 | seed=47 N=1500 iters=100000\n",
      "    BA=0.12921449760237005 | time=626.5s | rules=711\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.1276701348124374 | time=905.4s | rules=940\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.12737408142466342 | time=917.9s | rules=980\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.12849068863304408 | time=904.5s | rules=1012\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.1258296460176991 | time=907.5s | rules=1056\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.12638389526204033 | time=876.9s | rules=989\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.9}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.12804216059647053 | time=896.7s | rules=941\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.13125999034490157 | time=892.1s | rules=935\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.12661218333252247 | time=902.3s | rules=923\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.12937304060047017 | time=860.5s | rules=979\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.12705940158089182 | time=892.1s | rules=971\n",
      "Saved folds   -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_ISIC2019_GLCM_raw_folds_20260113_053413.jsonl\n",
      "Saved summary -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_ISIC2019_GLCM_raw_summary_20260113_053413.json\n",
      "\n",
      "ALL DONE. Final comparison CSV saved to:\n",
      "C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_multiclass_raw_feature_comparison_20260113_053413.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "# =====================================================\n",
    "# PROJECT & LIBRARY PATHS (Notebook-safe)\n",
    "# =====================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "EXSTRACS_ROOT = str(Path(PROJECT_ROOT) / \"..\" / \"Code\" / \"scikit-ExSTraCS-master\")\n",
    "if EXSTRACS_ROOT not in sys.path:\n",
    "    sys.path.insert(0, EXSTRACS_ROOT)\n",
    "\n",
    "\n",
    "from skExSTraCS.ExSTraCS import ExSTraCS\n",
    "\n",
    "# =====================================================\n",
    "# METRICS (MULTICLASS-SAFE)\n",
    "# =====================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Balanced Accuracy + Confusion Matrix\n",
    "    Fully supports multiclass classification\n",
    "    \"\"\"\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return bal_acc, cm\n",
    "\n",
    "# =====================================================\n",
    "# CROSS-VALIDATION CORE\n",
    "# =====================================================\n",
    "\n",
    "def run_cv(\n",
    "    csv_path,\n",
    "    dataset_name,\n",
    "    feature_family,\n",
    "    param_grid,\n",
    "    n_splits=5,\n",
    "    out_dir=\"lcs\"\n",
    "):\n",
    "    print(f\"\\n=== DATASET: {dataset_name} | FEATURES: {feature_family} ===\")\n",
    "\n",
    "    csv_path = os.path.join(PROJECT_ROOT, csv_path)\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Select features only (NO normalization here)\n",
    "    feature_cols = [c for c in data.columns if c not in (\"image\", \"label\")]\n",
    "    X = data[feature_cols].values.astype(float)\n",
    "    y = data[\"label\"].values.astype(int)\n",
    "\n",
    "    # -------------------------\n",
    "    # Safety check (informative)\n",
    "    # -------------------------\n",
    "    print(\n",
    "        f\"Feature range: min={np.min(X):.4f}, max={np.max(X):.4f} \"\n",
    "        \"(raw / un-normalized)\"\n",
    "    )\n",
    "\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    per_fold_records = []\n",
    "    summary_records = []\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(\"Params:\", params)\n",
    "        fold_scores = []\n",
    "\n",
    "        for fold, (tr_idx, te_idx) in enumerate(skf.split(X, y), start=1):\n",
    "            seed = 42 + fold\n",
    "\n",
    "            # =========================\n",
    "            # NO NORMALIZATION\n",
    "            # =========================\n",
    "            X_tr = X[tr_idx]\n",
    "            X_te = X[te_idx]\n",
    "            y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "            # =========================\n",
    "            # MODEL SETUP\n",
    "            # =========================\n",
    "            model = ExSTraCS()\n",
    "\n",
    "            model.N = params.get(\"N\", 2000)\n",
    "            model.learningIterations = params.get(\"learningIterations\", 100000)\n",
    "            model.theta_sel = params.get(\"theta_sel\", 0.8)\n",
    "\n",
    "            # Minority & specificity bias\n",
    "            model.nu = params.get(\"nu\", 3.0)\n",
    "            model.p_spec = params.get(\"p_spec\", 0.4)\n",
    "\n",
    "            # GA parameters\n",
    "            model.theta_GA = params.get(\"theta_GA\", 15)\n",
    "            model.chi = params.get(\"chi\", 0.8)\n",
    "            model.mu = params.get(\"mu\", 0.04)\n",
    "\n",
    "            model.doSubsumption = True\n",
    "            model.useBalancedAccuracy = True\n",
    "            model.randomSeed = seed\n",
    "\n",
    "            print(\n",
    "                f\"  Fold {fold} | seed={seed} \"\n",
    "                f\"N={model.N} iters={model.learningIterations}\"\n",
    "            )\n",
    "\n",
    "            start = time.time()\n",
    "            fit_exception = None\n",
    "\n",
    "            try:\n",
    "                model.fit(X_tr, y_tr)\n",
    "                y_pred = model.predict(X_te)\n",
    "                bal_acc, cm = compute_metrics(y_te, y_pred)\n",
    "            except Exception:\n",
    "                fit_exception = traceback.format_exc()\n",
    "                bal_acc = None\n",
    "                cm = None\n",
    "\n",
    "            duration = time.time() - start\n",
    "\n",
    "            try:\n",
    "                pop_size = len(model.population.popSet)\n",
    "            except Exception:\n",
    "                pop_size = None\n",
    "\n",
    "            print(\n",
    "                f\"    BA={bal_acc} | time={duration:.1f}s | rules={pop_size}\"\n",
    "            )\n",
    "\n",
    "            per_fold_records.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"feature_family\": feature_family,\n",
    "                \"params\": params,\n",
    "                \"fold\": fold,\n",
    "                \"balanced_accuracy\": bal_acc,\n",
    "                \"confusion_matrix\": cm.tolist() if cm is not None else None,\n",
    "                \"rule_population\": pop_size,\n",
    "                \"duration_seconds\": round(duration, 3),\n",
    "                \"fit_exception\": fit_exception\n",
    "            })\n",
    "\n",
    "            if bal_acc is not None:\n",
    "                fold_scores.append(bal_acc)\n",
    "\n",
    "        summary_records.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"feature_family\": feature_family,\n",
    "            \"params\": params,\n",
    "            \"mean_bal_acc\": float(np.mean(fold_scores)) if fold_scores else None,\n",
    "            \"std_bal_acc\": float(np.std(fold_scores)) if fold_scores else None,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    # =========================\n",
    "    # SAVE RESULTS\n",
    "    # =========================\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    folds_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_raw_folds_{ts}.jsonl\"\n",
    "    )\n",
    "    summary_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_raw_summary_{ts}.json\"\n",
    "    )\n",
    "\n",
    "    with open(folds_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        for rec in per_fold_records:\n",
    "            fh.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(summary_records, fh, indent=2)\n",
    "\n",
    "    print(f\"Saved folds   -> {folds_path}\")\n",
    "    print(f\"Saved summary -> {summary_path}\")\n",
    "\n",
    "    return pd.DataFrame(summary_records)\n",
    "\n",
    "# =====================================================\n",
    "# MAIN: MULTICLASS FEATURE FAMILY EXPERIMENTS\n",
    "# =====================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = [\n",
    "        {\"N\": 1500, \"learningIterations\": 100000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.9},\n",
    "    ]\n",
    "\n",
    "    experiments = [\n",
    "    (\"ham10000_lbp_multiclass.csv\",  \"HAM10000\", \"LBP\"),\n",
    "    (\"ham10000_glcm_multiclass.csv\", \"HAM10000\", \"GLCM\"),\n",
    "    (\"isic2019_lbp_multiclass.csv\",  \"ISIC2019\", \"LBP\"),\n",
    "    (\"isic2019_glcm_multiclass.csv\", \"ISIC2019\", \"GLCM\"),\n",
    "]\n",
    "\n",
    "\n",
    "    out_dir = os.path.join(PROJECT_ROOT, \"lcs\")\n",
    "    all_runs = []\n",
    "\n",
    "    for csv_path, dataset, feature_family in experiments:\n",
    "        df = run_cv(\n",
    "            csv_path=csv_path,\n",
    "            dataset_name=dataset,\n",
    "            feature_family=feature_family,\n",
    "            param_grid=param_grid,\n",
    "            n_splits=5,\n",
    "            out_dir=out_dir\n",
    "        )\n",
    "        all_runs.append(df)\n",
    "\n",
    "    final_df = pd.concat(all_runs, ignore_index=True)\n",
    "\n",
    "    final_out = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_multiclass_raw_feature_comparison_\"\n",
    "        f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    )\n",
    "\n",
    "    final_df.to_csv(final_out, index=False)\n",
    "    print(f\"\\nALL DONE. Final comparison CSV saved to:\\n{final_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9895ca",
   "metadata": {},
   "source": [
    "**Without Folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "feae2c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HAM10000 | GLCM ===\n",
      "Params: {'N': 1500, 'learningIterations': 100000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=1500 iters=100000\n",
      "    BA=0.14285714285714285 | time=622.5s | rules=976\n",
      "  Fold 2 | seed=44 N=1500 iters=100000\n",
      "    BA=0.14285714285714285 | time=650.4s | rules=995\n",
      "  Fold 3 | seed=45 N=1500 iters=100000\n",
      "    BA=0.14285714285714285 | time=546.8s | rules=1005\n",
      "  Fold 4 | seed=46 N=1500 iters=100000\n",
      "    BA=0.14854211834077605 | time=536.5s | rules=991\n",
      "  Fold 5 | seed=47 N=1500 iters=100000\n",
      "    BA=0.14285714285714285 | time=533.6s | rules=946\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.14285714285714285 | time=747.7s | rules=1365\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.14285714285714285 | time=755.6s | rules=1324\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.1447876447876448 | time=746.0s | rules=1365\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.14285714285714285 | time=750.7s | rules=1380\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.14285714285714285 | time=749.1s | rules=1352\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.9}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.14285714285714285 | time=745.5s | rules=1280\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.14285714285714285 | time=760.9s | rules=1394\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.14285714285714285 | time=744.0s | rules=1321\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.14285714285714285 | time=750.5s | rules=1309\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.1441383728379244 | time=758.3s | rules=1315\n",
      "Saved folds   -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_HAM10000_GLCM_folds_20260113_114556.jsonl\n",
      "Saved summary -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_HAM10000_GLCM_summary_20260113_114556.json\n",
      "\n",
      "=== ISIC2019 | LBP ===\n",
      "Params: {'N': 1500, 'learningIterations': 100000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=1500 iters=100000\n",
      "    BA=0.13024044179536434 | time=576.1s | rules=742\n",
      "  Fold 2 | seed=44 N=1500 iters=100000\n",
      "    BA=0.12565433689378552 | time=562.3s | rules=670\n",
      "  Fold 3 | seed=45 N=1500 iters=100000\n",
      "    BA=0.12977731766485134 | time=578.9s | rules=714\n",
      "  Fold 4 | seed=46 N=1500 iters=100000\n",
      "    BA=0.12570333600992772 | time=576.8s | rules=714\n",
      "  Fold 5 | seed=47 N=1500 iters=100000\n",
      "    BA=0.13076314069498002 | time=586.7s | rules=773\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.12733379336879452 | time=811.0s | rules=932\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.12653319764064033 | time=792.6s | rules=920\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.1292443554796802 | time=806.9s | rules=907\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.125 | time=953.2s | rules=1009\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.127277504995197 | time=902.9s | rules=914\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.9}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.12695971678377943 | time=934.6s | rules=1009\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.12825062533565038 | time=811.9s | rules=980\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.12677521603727393 | time=802.4s | rules=974\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.126092633126118 | time=852.3s | rules=906\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.12665308720292873 | time=847.0s | rules=936\n",
      "Saved folds   -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_ISIC2019_LBP_folds_20260113_145553.jsonl\n",
      "Saved summary -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_ISIC2019_LBP_summary_20260113_145553.json\n",
      "\n",
      "=== ISIC2019 | GLCM ===\n",
      "Params: {'N': 1500, 'learningIterations': 100000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=1500 iters=100000\n",
      "    BA=0.12605931499886333 | time=500.2s | rules=745\n",
      "  Fold 2 | seed=44 N=1500 iters=100000\n",
      "    BA=0.1380008582309714 | time=504.0s | rules=700\n",
      "  Fold 3 | seed=45 N=1500 iters=100000\n",
      "    BA=0.13023235458372712 | time=457.0s | rules=732\n",
      "  Fold 4 | seed=46 N=1500 iters=100000\n",
      "    BA=0.12748734081746166 | time=676.4s | rules=765\n",
      "  Fold 5 | seed=47 N=1500 iters=100000\n",
      "    BA=0.12701085789157143 | time=620.6s | rules=718\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.8}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.1292777449981226 | time=975.8s | rules=988\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.12968344170007284 | time=1162.2s | rules=988\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.13499838903685885 | time=1080.2s | rules=991\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.12545600996649195 | time=911.2s | rules=1027\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.1306807667796952 | time=865.0s | rules=1071\n",
      "Params: {'N': 2000, 'learningIterations': 120000, 'theta_sel': 0.9}\n",
      "  Fold 1 | seed=43 N=2000 iters=120000\n",
      "    BA=0.1344688969917149 | time=1312.9s | rules=965\n",
      "  Fold 2 | seed=44 N=2000 iters=120000\n",
      "    BA=0.12696722630477927 | time=1206.4s | rules=1050\n",
      "  Fold 3 | seed=45 N=2000 iters=120000\n",
      "    BA=0.12665193530372024 | time=1594.0s | rules=1046\n",
      "  Fold 4 | seed=46 N=2000 iters=120000\n",
      "    BA=0.12673546111821354 | time=1078.9s | rules=975\n",
      "  Fold 5 | seed=47 N=2000 iters=120000\n",
      "    BA=0.1271564889595326 | time=1020.3s | rules=984\n",
      "Saved folds   -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_ISIC2019_GLCM_folds_20260113_184840.jsonl\n",
      "Saved summary -> C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_ISIC2019_GLCM_summary_20260113_184840.json\n",
      "\n",
      "ALL DONE. Final comparison CSV saved to:\n",
      "C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\\lcs\\exstracs_feature_family_comparison_cleanNorm_20260113_184840.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "# =====================================================\n",
    "# PATH SETUP\n",
    "# =====================================================\n",
    "\n",
    "EXSTRACS_ROOT = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Code\\scikit-ExSTraCS-master\"\n",
    "DATA_ROOT = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\"\n",
    "\n",
    "if EXSTRACS_ROOT not in sys.path:\n",
    "    sys.path.insert(0, EXSTRACS_ROOT)\n",
    "\n",
    "from skExSTraCS.ExSTraCS import ExSTraCS\n",
    "\n",
    "# =====================================================\n",
    "# METRICS\n",
    "# =====================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return bal_acc, cm\n",
    "\n",
    "# =====================================================\n",
    "# CROSS-VALIDATION CORE\n",
    "# =====================================================\n",
    "\n",
    "def run_cv(csv_path, dataset_name, feature_family, param_grid,\n",
    "           n_splits=5, out_dir=\"lcs\"):\n",
    "\n",
    "    print(f\"\\n=== {dataset_name} | {feature_family} ===\")\n",
    "\n",
    "    csv_path = os.path.join(DATA_ROOT, csv_path)\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    feature_cols = [c for c in data.columns if c not in (\"image\", \"label\")]\n",
    "    X = data[feature_cols].values.astype(float)\n",
    "    y = data[\"label\"].values.astype(int)\n",
    "\n",
    "    # -------------------------\n",
    "    # SAFETY CHECK (OPTIONAL)\n",
    "    # -------------------------\n",
    "    if np.nanmin(X) < 0.0 or np.nanmax(X) > 1.0:\n",
    "        print(\"⚠ WARNING: Features not strictly in [0,1]. \"\n",
    "              \"Ensure preprocessing was applied correctly.\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_results = []\n",
    "    per_fold_records = []\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(\"Params:\", params)\n",
    "        fold_scores = []\n",
    "\n",
    "        for fold, (tr_idx, te_idx) in enumerate(skf.split(X, y), 1):\n",
    "            seed = 42 + fold\n",
    "\n",
    "            # =========================\n",
    "            # NO NORMALIZATION HERE\n",
    "            # =========================\n",
    "            X_tr = X[tr_idx]\n",
    "            X_te = X[te_idx]\n",
    "            y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "            # =========================\n",
    "            # MODEL SETUP\n",
    "            # =========================\n",
    "            model = ExSTraCS()\n",
    "            model.N = params.get(\"N\", 2000)\n",
    "            model.learningIterations = params.get(\"learningIterations\", 100000)\n",
    "            model.theta_sel = params.get(\"theta_sel\", 0.8)\n",
    "\n",
    "            # Minority & specificity bias\n",
    "            model.nu = params.get(\"nu\", 3.0)\n",
    "            model.p_spec = params.get(\"p_spec\", 0.4)\n",
    "            model.theta_GA = params.get(\"theta_GA\", 15)\n",
    "            model.chi = params.get(\"chi\", 0.8)\n",
    "            model.mu = params.get(\"mu\", 0.04)\n",
    "\n",
    "            model.doSubsumption = True\n",
    "            model.useBalancedAccuracy = True\n",
    "            model.randomSeed = seed\n",
    "\n",
    "            print(\n",
    "                f\"  Fold {fold} | seed={seed} \"\n",
    "                f\"N={model.N} iters={model.learningIterations}\"\n",
    "            )\n",
    "\n",
    "            start = time.time()\n",
    "            fit_exception = None\n",
    "\n",
    "            try:\n",
    "                model.fit(X_tr, y_tr)\n",
    "                y_pred = model.predict(X_te)\n",
    "                bal_acc, cm = compute_metrics(y_te, y_pred)\n",
    "            except Exception:\n",
    "                fit_exception = traceback.format_exc()\n",
    "                bal_acc = None\n",
    "                cm = None\n",
    "\n",
    "            duration = time.time() - start\n",
    "\n",
    "            try:\n",
    "                pop_size = len(model.population.popSet)\n",
    "            except Exception:\n",
    "                pop_size = None\n",
    "\n",
    "            print(\n",
    "                f\"    BA={bal_acc} | time={duration:.1f}s | rules={pop_size}\"\n",
    "            )\n",
    "\n",
    "            per_fold_records.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"feature_family\": feature_family,\n",
    "                \"params\": params,\n",
    "                \"fold\": fold,\n",
    "                \"balanced_accuracy\": bal_acc,\n",
    "                \"confusion_matrix\": cm.tolist() if cm is not None else None,\n",
    "                \"duration_seconds\": round(duration, 3),\n",
    "                \"rule_population\": pop_size,\n",
    "                \"fit_exception\": fit_exception\n",
    "            })\n",
    "\n",
    "            if bal_acc is not None:\n",
    "                fold_scores.append(bal_acc)\n",
    "\n",
    "        all_results.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"feature_family\": feature_family,\n",
    "            \"params\": params,\n",
    "            \"mean_bal_acc\": float(np.mean(fold_scores)) if fold_scores else None,\n",
    "            \"std_bal_acc\": float(np.std(fold_scores)) if fold_scores else None,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    # =========================\n",
    "    # SAVE RESULTS\n",
    "    # =========================\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    folds_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_folds_{ts}.jsonl\"\n",
    "    )\n",
    "    summary_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_summary_{ts}.json\"\n",
    "    )\n",
    "\n",
    "    with open(folds_path, \"w\") as fh:\n",
    "        for rec in per_fold_records:\n",
    "            fh.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    with open(summary_path, \"w\") as fh:\n",
    "        json.dump(all_results, fh, indent=2)\n",
    "\n",
    "    print(f\"Saved folds   -> {folds_path}\")\n",
    "    print(f\"Saved summary -> {summary_path}\")\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# =====================================================\n",
    "# MAIN\n",
    "# =====================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = [\n",
    "        {\"N\": 1500, \"learningIterations\": 100000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.9},\n",
    "    ]\n",
    "\n",
    "    experiments = [\n",
    "        (\"ham10000_glcm_multiclass_clean_norm.csv\", \"HAM10000\", \"GLCM\"),\n",
    "        (\"isic2019_lbp_multiclass_clean_norm.csv\",  \"ISIC2019\", \"LBP\"),\n",
    "        (\"isic2019_glcm_multiclass_clean_norm.csv\", \"ISIC2019\", \"GLCM\"),\n",
    "    ]\n",
    "\n",
    "    out_dir = os.path.join(DATA_ROOT, \"lcs\")\n",
    "\n",
    "    all_runs = []\n",
    "\n",
    "    for csv_path, dataset, feature_family in experiments:\n",
    "        df = run_cv(\n",
    "            csv_path=csv_path,\n",
    "            dataset_name=dataset,\n",
    "            feature_family=feature_family,\n",
    "            param_grid=param_grid,\n",
    "            n_splits=5,\n",
    "            out_dir=out_dir\n",
    "        )\n",
    "        all_runs.append(df)\n",
    "\n",
    "    final_df = pd.concat(all_runs, ignore_index=True)\n",
    "\n",
    "    final_out = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_feature_family_comparison_cleanNorm_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    )\n",
    "\n",
    "    final_df.to_csv(final_out, index=False)\n",
    "    print(f\"\\nALL DONE. Final comparison CSV saved to:\\n{final_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7ee53",
   "metadata": {},
   "source": [
    "Per Fold Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8edc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "# =====================================================\n",
    "# PATH SETUP\n",
    "# =====================================================\n",
    "\n",
    "EXSTRACS_ROOT = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Code\\scikit-ExSTraCS-master\"\n",
    "DATA_ROOT = r\"C:\\Users\\umair\\Videos\\PhD\\PhD Data\\Week 8 Jannuary\\Multiclass Classification\"\n",
    "\n",
    "if EXSTRACS_ROOT not in sys.path:\n",
    "    sys.path.insert(0, EXSTRACS_ROOT)\n",
    "\n",
    "from skExSTraCS.ExSTraCS import ExSTraCS\n",
    "\n",
    "# =====================================================\n",
    "# METRICS\n",
    "# =====================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return bal_acc, cm\n",
    "\n",
    "# =====================================================\n",
    "# MIN–MAX NORMALIZATION (TRAIN-ONLY)\n",
    "# =====================================================\n",
    "\n",
    "def minmax_fit(X):\n",
    "    \"\"\"\n",
    "    Fit Min–Max scaler on training data only.\n",
    "    \"\"\"\n",
    "    min_val = X.min(axis=0)\n",
    "    max_val = X.max(axis=0)\n",
    "    return min_val, max_val\n",
    "\n",
    "\n",
    "def minmax_transform(X, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Apply Min–Max normalization using precomputed stats.\n",
    "    \"\"\"\n",
    "    denom = (max_val - min_val)\n",
    "    denom[denom == 0] = 1.0  # prevent division by zero\n",
    "    return (X - min_val) / denom\n",
    "\n",
    "# =====================================================\n",
    "# CROSS-VALIDATION CORE\n",
    "# =====================================================\n",
    "\n",
    "def run_cv(csv_path, dataset_name, feature_family, param_grid,\n",
    "           n_splits=5, out_dir=\"lcs\"):\n",
    "\n",
    "    print(f\"\\n=== {dataset_name} | {feature_family} (Fold-wise Min–Max Norm) ===\")\n",
    "\n",
    "    csv_path = os.path.join(DATA_ROOT, csv_path)\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    feature_cols = [c for c in data.columns if c not in (\"image\", \"label\")]\n",
    "    X = data[feature_cols].values.astype(float)\n",
    "    y = data[\"label\"].values.astype(int)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_results = []\n",
    "    per_fold_records = []\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(\"Params:\", params)\n",
    "        fold_scores = []\n",
    "\n",
    "        for fold, (tr_idx, te_idx) in enumerate(skf.split(X, y), 1):\n",
    "            seed = 42 + fold\n",
    "\n",
    "            # =========================\n",
    "            # SPLIT\n",
    "            # =========================\n",
    "            X_tr_raw, X_te_raw = X[tr_idx], X[te_idx]\n",
    "            y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "            # =========================\n",
    "            # FOLD-WISE NORMALIZATION\n",
    "            # =========================\n",
    "            min_val, max_val = minmax_fit(X_tr_raw)\n",
    "            X_tr = minmax_transform(X_tr_raw, min_val, max_val)\n",
    "            X_te = minmax_transform(X_te_raw, min_val, max_val)\n",
    "\n",
    "            # =========================\n",
    "            # MODEL SETUP\n",
    "            # =========================\n",
    "            model = ExSTraCS()\n",
    "            model.N = params.get(\"N\", 2000)\n",
    "            model.learningIterations = params.get(\"learningIterations\", 100000)\n",
    "            model.theta_sel = params.get(\"theta_sel\", 0.8)\n",
    "\n",
    "            # Minority & specificity bias\n",
    "            model.nu = params.get(\"nu\", 3.0)\n",
    "            model.p_spec = params.get(\"p_spec\", 0.4)\n",
    "            model.theta_GA = params.get(\"theta_GA\", 15)\n",
    "            model.chi = params.get(\"chi\", 0.8)\n",
    "            model.mu = params.get(\"mu\", 0.04)\n",
    "\n",
    "            model.doSubsumption = True\n",
    "            model.useBalancedAccuracy = True\n",
    "            model.randomSeed = seed\n",
    "\n",
    "            print(\n",
    "                f\"  Fold {fold} | seed={seed} \"\n",
    "                f\"N={model.N} iters={model.learningIterations}\"\n",
    "            )\n",
    "\n",
    "            start = time.time()\n",
    "            fit_exception = None\n",
    "\n",
    "            try:\n",
    "                model.fit(X_tr, y_tr)\n",
    "                y_pred = model.predict(X_te)\n",
    "                bal_acc, cm = compute_metrics(y_te, y_pred)\n",
    "            except Exception:\n",
    "                fit_exception = traceback.format_exc()\n",
    "                bal_acc = None\n",
    "                cm = None\n",
    "\n",
    "            duration = time.time() - start\n",
    "\n",
    "            try:\n",
    "                pop_size = len(model.population.popSet)\n",
    "            except Exception:\n",
    "                pop_size = None\n",
    "\n",
    "            print(\n",
    "                f\"    BA={bal_acc} | time={duration:.1f}s | rules={pop_size}\"\n",
    "            )\n",
    "\n",
    "            per_fold_records.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"feature_family\": feature_family,\n",
    "                \"params\": params,\n",
    "                \"fold\": fold,\n",
    "                \"balanced_accuracy\": bal_acc,\n",
    "                \"confusion_matrix\": cm.tolist() if cm is not None else None,\n",
    "                \"duration_seconds\": round(duration, 3),\n",
    "                \"rule_population\": pop_size,\n",
    "                \"fit_exception\": fit_exception\n",
    "            })\n",
    "\n",
    "            if bal_acc is not None:\n",
    "                fold_scores.append(bal_acc)\n",
    "\n",
    "        all_results.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"feature_family\": feature_family,\n",
    "            \"params\": params,\n",
    "            \"mean_bal_acc\": float(np.mean(fold_scores)) if fold_scores else None,\n",
    "            \"std_bal_acc\": float(np.std(fold_scores)) if fold_scores else None,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    # =========================\n",
    "    # SAVE RESULTS\n",
    "    # =========================\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    folds_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_foldNorm_folds_{ts}.jsonl\"\n",
    "    )\n",
    "    summary_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_{dataset_name}_{feature_family}_foldNorm_summary_{ts}.json\"\n",
    "    )\n",
    "\n",
    "    with open(folds_path, \"w\") as fh:\n",
    "        for rec in per_fold_records:\n",
    "            fh.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    with open(summary_path, \"w\") as fh:\n",
    "        json.dump(all_results, fh, indent=2)\n",
    "\n",
    "    print(f\"Saved folds   -> {folds_path}\")\n",
    "    print(f\"Saved summary -> {summary_path}\")\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# =====================================================\n",
    "# MAIN\n",
    "# =====================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    param_grid = [\n",
    "        {\"N\": 1500, \"learningIterations\": 100000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.8},\n",
    "        {\"N\": 2000, \"learningIterations\": 120000, \"theta_sel\": 0.9},\n",
    "    ]\n",
    "\n",
    "    experiments = [\n",
    "        (\"ham10000_lbp_multiclass_clean_norm.csv\",  \"HAM10000\", \"LBP\"),\n",
    "        (\"ham10000_glcm_multiclass_clean_norm.csv\", \"HAM10000\", \"GLCM\"),\n",
    "        (\"isic2019_lbp_multiclass_clean_norm.csv\",  \"ISIC2019\", \"LBP\"),\n",
    "        (\"isic2019_glcm_multiclass_clean_norm.csv\", \"ISIC2019\", \"GLCM\"),\n",
    "    ]\n",
    "\n",
    "    out_dir = os.path.join(DATA_ROOT, \"lcs\")\n",
    "\n",
    "    all_runs = []\n",
    "\n",
    "    for csv_path, dataset, feature_family in experiments:\n",
    "        df = run_cv(\n",
    "            csv_path=csv_path,\n",
    "            dataset_name=dataset,\n",
    "            feature_family=feature_family,\n",
    "            param_grid=param_grid,\n",
    "            n_splits=5,\n",
    "            out_dir=out_dir\n",
    "        )\n",
    "        all_runs.append(df)\n",
    "\n",
    "    final_df = pd.concat(all_runs, ignore_index=True)\n",
    "\n",
    "    final_out = os.path.join(\n",
    "        out_dir,\n",
    "        f\"exstracs_feature_family_comparison_foldNorm_\"\n",
    "        f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    )\n",
    "\n",
    "    final_df.to_csv(final_out, index=False)\n",
    "    print(f\"\\nALL DONE. Final comparison CSV saved to:\\n{final_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
